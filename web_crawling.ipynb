{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def GetAllinks(url, urls, d):\n",
    "    q=[(url,0)]\n",
    "    while q :\n",
    "        url,currDept=q.pop(0)\n",
    "        # add_row={'url':url}\n",
    "        urls.append(url)\n",
    "        # print(currDept)\n",
    "        if currDept==d:\n",
    "            continue\n",
    "        reqs = requests.get(url)\n",
    "        soup = BeautifulSoup(reqs.text, 'html.parser')\n",
    "        for link in soup.find_all('a'):\n",
    "            gotLink = link.get('href')\n",
    "            \n",
    "            if(gotLink !=None):\n",
    "                if 'http' in gotLink:\n",
    "                    if 'python' in gotLink:\n",
    "                        if gotLink not in urls:\n",
    "                            q.append((gotLink,currDept+1))\n",
    "                   \n",
    "                else:\n",
    "                    q.append((url+gotLink,currDept+1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetAllinks2(url, urls, d):\n",
    "    q = [(url, 0)]\n",
    "    visited_urls = set()\n",
    "\n",
    "    while q:\n",
    "        url, currDept = q.pop(0)\n",
    "\n",
    "        # Check if the URL has already been visited\n",
    "        if url in visited_urls:\n",
    "            continue\n",
    "\n",
    "        visited_urls.add(url)\n",
    "        urls.append(url)\n",
    "        # print(currDept)\n",
    "\n",
    "        if currDept == d:\n",
    "            continue\n",
    "\n",
    "        reqs = requests.get(url)\n",
    "        soup = BeautifulSoup(reqs.text, 'html.parser')\n",
    "\n",
    "        for link in soup.find_all('a'):\n",
    "            gotLink = link.get('href')\n",
    "\n",
    "            if gotLink is not None:\n",
    "                if 'http' in gotLink and 'python' in gotLink:\n",
    "                    if gotLink not in visited_urls:\n",
    "                        q.append((gotLink, currDept + 1))\n",
    "                else:\n",
    "                    q.append((url + gotLink, currDept + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_url=\"https://www.geeksforgeeks.org/python3-tutorial/\"\n",
    "links=[]\n",
    "GetAllinks(start_url,links,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### removing duplicate links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25844\n",
      "4990\n",
      "4990 {'url': 'https://www.geeksforgeeks.org/python-list-exercise/?ref=dhm#'}\n"
     ]
    }
   ],
   "source": [
    "print(len(links))\n",
    "unique_links = list(set(links))\n",
    "print(len(unique_links))\n",
    "# add url tag to url\n",
    "unique_url_links=[]\n",
    "for link in unique_links:\n",
    "    unique_url_links.append({'url':link})\n",
    "print(len(unique_url_links),unique_url_links[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add title to links\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# def GetAllinks(url, urls, d):\n",
    "#     q = [(url, 0)]\n",
    "\n",
    "#     while q:\n",
    "#         url, currDept = q.pop(0)\n",
    "#         add_to_array={'url':url,\n",
    "#                       'title':GetPageTitle(url)\n",
    "#                       }\n",
    "#         urls.append(add_to_array)  # Append page title along with the URL\n",
    "#         print(currDept)\n",
    "#         if len(urls)>10:\n",
    "#             return\n",
    "#         if currDept == d:\n",
    "#             continue\n",
    "\n",
    "#         reqs = requests.get(url)\n",
    "#         soup = BeautifulSoup(reqs.text, 'html.parser')\n",
    "\n",
    "#         for link in soup.find_all('a'):\n",
    "#             gotLink = link.get('href')\n",
    "\n",
    "#             if gotLink is not None:\n",
    "#                 if 'http' in gotLink:\n",
    "#                     if 'python' in gotLink:\n",
    "#                         q.append((gotLink, currDept + 1))\n",
    "#                 else:\n",
    "#                     q.append((url + gotLink, currDept + 1))\n",
    "\n",
    "# def GetPageTitle(url):\n",
    "#     try:\n",
    "#         reqs = requests.get(url)\n",
    "#         soup = BeautifulSoup(reqs.text, 'html.parser')\n",
    "#         title = soup.title.string\n",
    "#         return title\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error fetching page title for {url}: {e}\")\n",
    "#         return \"Page title not available\"\n",
    "\n",
    "# # Example usage:\n",
    "# urls = []\n",
    "# GetAllinks(start_url, urls, 1)\n",
    "# for url in urls:\n",
    "#     print(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.geeksforgeeks.org/python3-if-if-else-nested-if-if-elif-statements/?ref=dhm'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links[3600]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## storing json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_json(content, path, file_name):\n",
    "    def add_url_tag(myset):\n",
    "        # mylist=list(myset)\n",
    "        save=[]\n",
    "        for link in myset:\n",
    "            save.append({'link':link})\n",
    "        return save\n",
    "    content=add_url_tag(content)\n",
    "    try:\n",
    "        # Ensure the specified directory exists\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "\n",
    "        # Store the content in a JSON file\n",
    "        with open(os.path.join(path, file_name), 'w') as json_file:\n",
    "            json.dump(content, json_file)\n",
    "\n",
    "        print(f'The content has been stored in the JSON file: {file_name}')\n",
    "    except Exception as e:\n",
    "        print(f'Error storing JSON content: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'store_json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mstore_json\u001b[49m(unique_url_links,path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnew_data\u001b[39m\u001b[38;5;124m'\u001b[39m,file_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeeks_for_geeks_depth5.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m unique_url_links[\u001b[38;5;28mlen\u001b[39m(unique_url_links)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'store_json' is not defined"
     ]
    }
   ],
   "source": [
    "store_json(unique_url_links,path='new_data',file_name='geeks_for_geeks_depth5.json')\n",
    "unique_url_links[len(unique_url_links)-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only considering unique links for crwaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetAllinks_set(url, urls_set, d):\n",
    "    q=[(url,0)]\n",
    "    aa,bb=1,1\n",
    "    while q :\n",
    "        try:\n",
    "            url,currDept=q.pop(0)\n",
    "            # add_row={'url':url}\n",
    "            if url in urls_set:\n",
    "                continue\n",
    "            urls_set.add(url)\n",
    "            # print(currDept)\n",
    "            if currDept==d:\n",
    "                continue\n",
    "            reqs = requests.get(url)\n",
    "            soup = BeautifulSoup(reqs.text, 'html.parser')\n",
    "            for link in soup.find_all('a'):\n",
    "                try:\n",
    "                    gotLink = link.get('href')\n",
    "                    \n",
    "                    if(gotLink !=None):\n",
    "                        if 'http' in gotLink:\n",
    "                            if 'python' in gotLink:\n",
    "                                # if gotLink not in urls:\n",
    "                                q.append((gotLink,currDept+1))\n",
    "                        \n",
    "                        else:\n",
    "                            q.append((url+gotLink,currDept+1))\n",
    "                except KeyboardInterrupt:\n",
    "                    print(\"Inner for loop error :\",aa)\n",
    "                    aa+=1\n",
    "                    continue\n",
    "        except (requests.exceptions.RequestException,KeyboardInterrupt) as e:\n",
    "            print(\"outer while loop error : \",bb,\" : \",e)\n",
    "            bb+=1\n",
    "            return urls_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetAllinks_set2(url, urls_set, d):\n",
    "    q = [(url, 0)]\n",
    "    collected_data = set()\n",
    "\n",
    "    while q:\n",
    "        url, currDept = q.pop(0)\n",
    "\n",
    "        # Check if the URL has already been visited\n",
    "        if url in urls_set:\n",
    "            continue\n",
    "\n",
    "        urls_set.add(url)\n",
    "        collected_data.add(url)  # Save the data\n",
    "\n",
    "        if currDept == d:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            reqs = requests.get(url)\n",
    "            reqs.raise_for_status()  # Check for HTTP errors\n",
    "\n",
    "            soup = BeautifulSoup(reqs.text, 'html.parser')\n",
    "\n",
    "            for link in soup.find_all('a'):\n",
    "                gotLink = link.get('href')\n",
    "\n",
    "                if gotLink is not None:\n",
    "                    if 'http' in gotLink:\n",
    "                        if 'python' in gotLink:\n",
    "                            if gotLink not in urls_set:\n",
    "                                q.append((gotLink, currDept + 1))\n",
    "                    else:\n",
    "                        q.append((url + gotLink, currDept + 1))\n",
    "\n",
    "        except (requests.exceptions.RequestException,KeyboardInterrupt) as e :\n",
    "            print(f\"Connection error: {e}\")\n",
    "            return collected_data  # End the function and return the collected data\n",
    "\n",
    "    return collected_data  # Return the collected data if the function completes successfully\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links2=set()\n",
    "start_url=\"https://www.geeksforgeeks.org/python3-tutorial/\"\n",
    "links_result=GetAllinks_set(start_url,links2,3)\n",
    "# print(\"Returned =\",len(links_result))\n",
    "print(len(links2))\n",
    "links2\n",
    "store_json(links2,path='new_data',file_name='geeks_for_geeks_depth5_new.json')\n",
    "df=pd.read_json('./new_data/geeks_for_geeks_depth5_new.json')\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The content has been stored in the JSON file: python_tutorial.json\n"
     ]
    }
   ],
   "source": [
    "df2=pd.read_csv('./python_tutorial_links.txt', header=None)\n",
    "link_set=[]\n",
    "for link in df2:\n",
    "    link_set.append(df2[link][0])\n",
    "    # print(link)\n",
    "len(link_set)\n",
    "store_json(link_set,path='new_data/starting_urls',file_name='python_tutorial.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40957 4990 4990 5972\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "40961"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_to_set(path,get_label):\n",
    "    res=set()\n",
    "    data=pd.read_json(path)\n",
    "    # print(data['link'])\n",
    "    for index,row in data.iterrows():\n",
    "        res.add(row[get_label])\n",
    "    return res\n",
    "d3=add_to_set('./new_data/geeks_for_geeks_d3_41k.json','link')\n",
    "d2=add_to_set('./new_data/geeks_for_geeks_depth2.json','url')\n",
    "d5=add_to_set('./new_data/geeks_for_geeks_depth5.json','link')\n",
    "d5n=add_to_set('./new_data/geeks_for_geeks_depth5_new.json','link')\n",
    "print(len(d3),len(d2),len(d5),len(d5n))\n",
    "d23=d3.union(d2)\n",
    "d235=d23.union(d5)\n",
    "d235n=d235.union(d5n)\n",
    "len(d235n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The content has been stored in the JSON file: geeks_for_geeks41k_final.json\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "search_engine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
