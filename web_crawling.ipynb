{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def GetAllinks(url, urls, d):\n",
    "    q=[(url,0)]\n",
    "    while q :\n",
    "        url,currDept=q.pop(0)\n",
    "        # add_row={'url':url}\n",
    "        urls.append(url)\n",
    "        # print(currDept)\n",
    "        if currDept==d:\n",
    "            continue\n",
    "        reqs = requests.get(url)\n",
    "        soup = BeautifulSoup(reqs.text, 'html.parser')\n",
    "        for link in soup.find_all('a'):\n",
    "            gotLink = link.get('href')\n",
    "            \n",
    "            if(gotLink !=None):\n",
    "                if 'http' in gotLink:\n",
    "                    if 'python' in gotLink:\n",
    "                        if gotLink not in urls:\n",
    "                            q.append((gotLink,currDept+1))\n",
    "                   \n",
    "                else:\n",
    "                    q.append((url+gotLink,currDept+1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetAllinks2(url, urls, d):\n",
    "    q = [(url, 0)]\n",
    "    visited_urls = set()\n",
    "\n",
    "    while q:\n",
    "        url, currDept = q.pop(0)\n",
    "\n",
    "        # Check if the URL has already been visited\n",
    "        if url in visited_urls:\n",
    "            continue\n",
    "\n",
    "        visited_urls.add(url)\n",
    "        urls.append(url)\n",
    "        # print(currDept)\n",
    "\n",
    "        if currDept == d:\n",
    "            continue\n",
    "\n",
    "        reqs = requests.get(url)\n",
    "        soup = BeautifulSoup(reqs.text, 'html.parser')\n",
    "\n",
    "        for link in soup.find_all('a'):\n",
    "            gotLink = link.get('href')\n",
    "\n",
    "            if gotLink is not None:\n",
    "                if 'http' in gotLink and 'python' in gotLink:\n",
    "                    if gotLink not in visited_urls:\n",
    "                        q.append((gotLink, currDept + 1))\n",
    "                else:\n",
    "                    q.append((url + gotLink, currDept + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_url=\"https://www.geeksforgeeks.org/python3-tutorial/\"\n",
    "links=[]\n",
    "GetAllinks(start_url,links,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### removing duplicate links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25844\n",
      "4990\n",
      "4990 {'url': 'https://www.geeksforgeeks.org/python-list-exercise/?ref=dhm#'}\n"
     ]
    }
   ],
   "source": [
    "print(len(links))\n",
    "unique_links = list(set(links))\n",
    "print(len(unique_links))\n",
    "# add url tag to url\n",
    "unique_url_links=[]\n",
    "for link in unique_links:\n",
    "    unique_url_links.append({'url':link})\n",
    "print(len(unique_url_links),unique_url_links[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add title to links\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# def GetAllinks(url, urls, d):\n",
    "#     q = [(url, 0)]\n",
    "\n",
    "#     while q:\n",
    "#         url, currDept = q.pop(0)\n",
    "#         add_to_array={'url':url,\n",
    "#                       'title':GetPageTitle(url)\n",
    "#                       }\n",
    "#         urls.append(add_to_array)  # Append page title along with the URL\n",
    "#         print(currDept)\n",
    "#         if len(urls)>10:\n",
    "#             return\n",
    "#         if currDept == d:\n",
    "#             continue\n",
    "\n",
    "#         reqs = requests.get(url)\n",
    "#         soup = BeautifulSoup(reqs.text, 'html.parser')\n",
    "\n",
    "#         for link in soup.find_all('a'):\n",
    "#             gotLink = link.get('href')\n",
    "\n",
    "#             if gotLink is not None:\n",
    "#                 if 'http' in gotLink:\n",
    "#                     if 'python' in gotLink:\n",
    "#                         q.append((gotLink, currDept + 1))\n",
    "#                 else:\n",
    "#                     q.append((url + gotLink, currDept + 1))\n",
    "\n",
    "# def GetPageTitle(url):\n",
    "#     try:\n",
    "#         reqs = requests.get(url)\n",
    "#         soup = BeautifulSoup(reqs.text, 'html.parser')\n",
    "#         title = soup.title.string\n",
    "#         return title\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error fetching page title for {url}: {e}\")\n",
    "#         return \"Page title not available\"\n",
    "\n",
    "# # Example usage:\n",
    "# urls = []\n",
    "# GetAllinks(start_url, urls, 1)\n",
    "# for url in urls:\n",
    "#     print(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.geeksforgeeks.org/python3-if-if-else-nested-if-if-elif-statements/?ref=dhm'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links[3600]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## storing json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_json(content, path, file_name):\n",
    "    def add_url_tag(myset):\n",
    "        # mylist=list(myset)\n",
    "        save=[]\n",
    "        for link in myset:\n",
    "            save.append({'link':link})\n",
    "        return save\n",
    "    content=add_url_tag(content)\n",
    "    try:\n",
    "        # Ensure the specified directory exists\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "\n",
    "        # Store the content in a JSON file\n",
    "        with open(os.path.join(path, file_name), 'w') as json_file:\n",
    "            json.dump(content, json_file)\n",
    "\n",
    "        print(f'The content has been stored in the JSON file: {file_name}')\n",
    "    except Exception as e:\n",
    "        print(f'Error storing JSON content: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'store_json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mstore_json\u001b[49m(unique_url_links,path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnew_data\u001b[39m\u001b[38;5;124m'\u001b[39m,file_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeeks_for_geeks_depth5.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m unique_url_links[\u001b[38;5;28mlen\u001b[39m(unique_url_links)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'store_json' is not defined"
     ]
    }
   ],
   "source": [
    "store_json(unique_url_links,path='new_data',file_name='geeks_for_geeks_depth5.json')\n",
    "unique_url_links[len(unique_url_links)-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only considering unique links for crwaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "\n",
    "def check_page_validity(reqs,url):\n",
    "    if not (200 <= reqs.status_code < 400):\n",
    "        print(f\"HTTP Error {reqs.status_code}: {url}\") \n",
    "        return False\n",
    "    else:\n",
    "        error_keywords = [\"page not found\"]  # Customize based on your needs\n",
    "\n",
    "        for keyword in error_keywords:\n",
    "            if keyword in reqs.text.lower():\n",
    "                print(f\"Potential error detected: {keyword}\")\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "def GetAllinks_set(url, urls_set, d,SameDomain=False):\n",
    "    q=[(url,0)]\n",
    "    aa,bb=1,1\n",
    "    count=0\n",
    "    domain= urlparse(url).netloc\n",
    "    try:\n",
    "        while q :\n",
    "            try:\n",
    "                \n",
    "                url,currDept=q.pop(0)\n",
    "                # add_row={'url':url}\n",
    "                \n",
    "                count+=1\n",
    "                if count % 100==0:\n",
    "                    print(count,\"\\t\",url)\n",
    "                # print(currDept)\n",
    "                if currDept==d:continue\n",
    "                reqs = requests.get(url)\n",
    "                if not check_page_validity(reqs,url):continue\n",
    "                soup = BeautifulSoup(reqs.text, 'html.parser')\n",
    "                for link in soup.find_all('a'):\n",
    "                    try:\n",
    "                        \n",
    "                            gotLink = link.get('href')\n",
    "                            if(gotLink !=None):\n",
    "                                absLink=urljoin(url,gotLink)\n",
    "                                if(SameDomain):\n",
    "                                    parsed_url = urlparse(absLink)\n",
    "                                    if parsed_url.netloc != domain:\n",
    "                                        continue\n",
    "                                if 'python' in absLink and absLink not in urls_set:  \n",
    "                                    urls_set.add(absLink)  \n",
    "                                    q.append((absLink,currDept+1))\n",
    "                            \n",
    "                    except KeyboardInterrupt:\n",
    "                        print(\"Inner for loop error :\",aa)\n",
    "                        aa+=1\n",
    "                        continue\n",
    "            except (requests.exceptions.RequestException) as e:\n",
    "                print(\"outer while loop error : \",bb,\" : \",e)\n",
    "                bb+=1\n",
    "                continue\n",
    "    except (requests.exceptions.RequestException,KeyboardInterrupt) as e:\n",
    "        print(\"outer most error : \",\" : \",e)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetAllinks_set2(url, urls_set, d):\n",
    "    q = [(url, 0)]\n",
    "    collected_data = set()\n",
    "\n",
    "    while q:\n",
    "        url, currDept = q.pop(0)\n",
    "\n",
    "        # Check if the URL has already been visited\n",
    "        if url in urls_set:\n",
    "            continue\n",
    "\n",
    "        urls_set.add(url)\n",
    "        collected_data.add(url)  # Save the data\n",
    "\n",
    "        if currDept == d:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            reqs = requests.get(url)\n",
    "            reqs.raise_for_status()  # Check for HTTP errors\n",
    "\n",
    "            soup = BeautifulSoup(reqs.text, 'html.parser')\n",
    "\n",
    "            for link in soup.find_all('a'):\n",
    "                gotLink = link.get('href')\n",
    "\n",
    "                if gotLink is not None:\n",
    "                    if 'http' in gotLink:\n",
    "                        if 'python' in gotLink:\n",
    "                            if gotLink not in urls_set:\n",
    "                                q.append((gotLink, currDept + 1))\n",
    "                    else:\n",
    "                        q.append((url + gotLink, currDept + 1))\n",
    "\n",
    "        except (requests.exceptions.RequestException,KeyboardInterrupt) as e :\n",
    "            print(f\"Connection error: {e}\")\n",
    "            return collected_data  # End the function and return the collected data\n",
    "\n",
    "    return collected_data  # Return the collected data if the function completes successfully\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 \t https://www.javatpoint.com/how-to-read-json-file-in-python\n",
      "200 \t https://www.javatpoint.com/namedtuple-in-python\n",
      "HTTP Error 520: https://www.javatpoint.com/python-coroutines\n",
      "300 \t https://www.javatpoint.com/code-template-for-creating-objects-in-python\n",
      "400 \t https://www.javatpoint.com/how-to-make-an-area-plot-in-python-using-bokeh\n",
      "500 \t https://www.javatpoint.com/library-in-python\n",
      "600 \t https://www.javatpoint.com/python-new-line\n",
      "700 \t https://www.javatpoint.com/draw-great-indian-flag-using-python-code\n",
      "800 \t https://www.javatpoint.com/image-viewer-application-using-pyqt5-in-python\n",
      "900 \t https://www.javatpoint.com/new-features-and-fixes-in-python-3-11\n",
      "HTTP Error 503: https://www.javatpoint.com/visualizing-dicom-images-using-pydicom-and-matplotlib-in-python\n",
      "HTTP Error 503: https://www.javatpoint.com/validating-entry-widget-in-python-tkinter\n",
      "HTTP Error 503: https://www.javatpoint.com/random-shuffle-python\n",
      "HTTP Error 503: https://www.javatpoint.com/new-in-python\n",
      "HTTP Error 503: https://www.javatpoint.com/build-a-whatsapp-flashcard-app-with-twilio-flask-and-python\n",
      "HTTP Error 503: https://www.javatpoint.com/control-structures-in-python\n",
      "HTTP Error 503: https://www.javatpoint.com/define-a-python-class-for-complex-numbers\n",
      "HTTP Error 503: https://www.javatpoint.com/find-lower-insertion-point-in-python\n",
      "HTTP Error 503: https://www.javatpoint.com/finding-element-in-rotated-sorted-array-in-python\n",
      "HTTP Error 503: https://www.javatpoint.com/first-occurrence-using-binary-search-in-python\n",
      "HTTP Error 520: https://www.javatpoint.com/hybrid-programming-using-python-and-dart\n",
      "1000 \t https://www.javatpoint.com/multidimensional-image-processing-using-scipy-in-python\n",
      "1100 \t https://www.javatpoint.com/interface-in-python\n",
      "1200 \t https://www.javatpoint.com/lomax-distribution-in-statistics-using-python\n",
      "1300 \t https://www.javatpoint.com/screen-manager-in-kivy-using-kv-file-in-python\n",
      "1400 \t https://www.javatpoint.com/python-check-number-is-odd-or-even\n",
      "1500 \t https://www.javatpoint.com/python-hasattr-function\n",
      "1600 \t https://www.javatpoint.com/python-openpyxl#Iteratebyrows\n",
      "1626\n"
     ]
    }
   ],
   "source": [
    "links2=set()\n",
    "start_url=\"https://www.javatpoint.com/python-tutorial\"\n",
    "GetAllinks_set(start_url,links2,2,True)\n",
    "# print(\"Returned =\",len(links_result))\n",
    "print(len(links2))\n",
    "\n",
    "# store_json(links2,path='new_data',file_name='javatpoint.json')\n",
    "# df=pd.read_json('./new_data/javatpoint.json')\n",
    "# len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The content has been stored in the JSON file: javatpoint.json\n"
     ]
    }
   ],
   "source": [
    "store_json(links2,path='new_data',file_name='javatpoint.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'store_json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# print(link)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mlen\u001b[39m(link_set)\n\u001b[1;32m----> 7\u001b[0m \u001b[43mstore_json\u001b[49m(link_set,path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnew_data/starting_urls\u001b[39m\u001b[38;5;124m'\u001b[39m,file_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpython_tutorial.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'store_json' is not defined"
     ]
    }
   ],
   "source": [
    "df2=pd.read_csv('./python_tutorial_links.txt', header=None)\n",
    "link_set=[]\n",
    "for link in df2:\n",
    "    link_set.append(df2[link][0])\n",
    "    # print(link)\n",
    "len(link_set)\n",
    "store_json(link_set,path='new_data/starting_urls',file_name='python_tutorial.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40957 4990 4990 5972\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "40961"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_to_set(path,get_label):\n",
    "    res=set()\n",
    "    data=pd.read_json(path)\n",
    "    # print(data['link'])\n",
    "    for index,row in data.iterrows():\n",
    "        res.add(row[get_label])\n",
    "    return res\n",
    "d3=add_to_set('./new_data/geeks_for_geeks_d3_41k.json','link')\n",
    "d2=add_to_set('./new_data/geeks_for_geeks_depth2.json','url')\n",
    "d5=add_to_set('./new_data/geeks_for_geeks_depth5.json','link')\n",
    "d5n=add_to_set('./new_data/geeks_for_geeks_depth5_new.json','link')\n",
    "print(len(d3),len(d2),len(d5),len(d5n))\n",
    "d23=d3.union(d2)\n",
    "d235=d23.union(d5)\n",
    "d235n=d235.union(d5n)\n",
    "len(d235n)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
  },
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
